{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.edmundson import EdmundsonSummarizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.summarizers.kl import KLSummarizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import io\n",
    "import json\n",
    "from rouge import Rouge\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Edmundson Summary:\n",
      "Natural language processing is a subfield of artificial intelligence (AI) focused on the interaction between computers and humans through natural language.\n",
      "The ultimate objective of NLP is to enable computers to understand, interpret, and generate human languages in a way that is both valuable and meaningful.\n",
      "NLP is used to apply algorithms to identify and extract the natural language rules such that the unstructured language data is converted into a form that computers can understand.\n",
      "\n",
      "LSA Summary:\n",
      "Natural language processing is a subfield of artificial intelligence (AI) focused on the interaction between computers and humans through natural language.\n",
      "The ultimate objective of NLP is to enable computers to understand, interpret, and generate human languages in a way that is both valuable and meaningful.\n",
      "NLP is used to apply algorithms to identify and extract the natural language rules such that the unstructured language data is converted into a form that computers can understand.\n",
      "\n",
      "KL Summary:\n",
      "Natural language processing is a subfield of artificial intelligence (AI) focused on the interaction between computers and humans through natural language.\n",
      "The ultimate objective of NLP is to enable computers to understand, interpret, and generate human languages in a way that is both valuable and meaningful.\n",
      "NLP is used to apply algorithms to identify and extract the natural language rules such that the unstructured language data is converted into a form that computers can understand.\n",
      "\n",
      "LexRank Summary:\n",
      "Natural language processing is a subfield of artificial intelligence (AI) focused on the interaction between computers and humans through natural language.\n",
      "The ultimate objective of NLP is to enable computers to understand, interpret, and generate human languages in a way that is both valuable and meaningful.\n",
      "NLP is used to apply algorithms to identify and extract the natural language rules such that the unstructured language data is converted into a form that computers can understand.\n",
      "\n",
      "File Summaries:\n",
      "\n",
      "Edmundson Summary:\n",
      "Natural language processing is a subfield of artificial intelligence (AI) focused on the interaction between computers and humans through natural language.\n",
      "The ultimate objective of NLP is to enable computers to understand, interpret, and generate human languages in a way that is both valuable and meaningful.\n",
      "NLP is used to apply algorithms to identify and extract the natural language rules such that the unstructured language data is converted into a form that computers can understand.\n",
      "\n",
      "LSA Summary:\n",
      "Natural language processing is a subfield of artificial intelligence (AI) focused on the interaction between computers and humans through natural language.\n",
      "The ultimate objective of NLP is to enable computers to understand, interpret, and generate human languages in a way that is both valuable and meaningful.\n",
      "NLP is used to apply algorithms to identify and extract the natural language rules such that the unstructured language data is converted into a form that computers can understand.\n",
      "\n",
      "KL Summary:\n",
      "Natural language processing is a subfield of artificial intelligence (AI) focused on the interaction between computers and humans through natural language.\n",
      "The ultimate objective of NLP is to enable computers to understand, interpret, and generate human languages in a way that is both valuable and meaningful.\n",
      "NLP is used to apply algorithms to identify and extract the natural language rules such that the unstructured language data is converted into a form that computers can understand.\n",
      "\n",
      "LexRank Summary:\n",
      "Natural language processing is a subfield of artificial intelligence (AI) focused on the interaction between computers and humans through natural language.\n",
      "The ultimate objective of NLP is to enable computers to understand, interpret, and generate human languages in a way that is both valuable and meaningful.\n",
      "NLP is used to apply algorithms to identify and extract the natural language rules such that the unstructured language data is converted into a form that computers can understand.\n",
      "\n",
      "URL Summaries:\n",
      "\n",
      "Edmundson Summary:\n",
      "{\\displaystyle {RMM(token_{N})}={PMM(token_{N})}\\times {\\frac {1}{2d}}\\left(\\sum _{i=-d}^{d}{((PMM(token_{N})}\\times {PF(token_{N-i},token_{N},token_{N+i}))_{i}}\\right)}\n",
      "1 the Road Artificial intelligence detection software Automated essay scoring Biomedical text mining Compound term processing Computational linguistics Computer-assisted reviewing Controlled natural language Deep learning Deep linguistic processing Distributional semantics Foreign language reading aid Foreign language writing aid Information extraction Information retrieval Language and Communication Technologies Language model Language technology Latent semantic indexing Multi-agent system Native-language identification Natural-language programming Natural-language understanding Natural-language search Outline of natural language processing Query expansion Query understanding Reification (linguistics) Speech processing Spoken dialogue systems Text-proofing Text simplification Transformer (machine learning model) Truecasing Question answering Word2vec\n",
      "External links[edit] Media related to Natural language processing at Wikimedia Commons vteNatural language processingGeneral terms AI-complete Bag-of-words n-gram Bigram Trigram Computational linguistics Natural language understanding Stop words Text processing Text analysis Argument mining Collocation extraction Concept mining Coreference resolution Deep linguistic processing Distant reading Information extraction Named-entity recognition Ontology learning Parsing Semantic parsing Syntactic parsing Part-of-speech tagging Semantic analysis Semantic role labeling Semantic decomposition Semantic similarity Sentiment analysis Terminology extraction Text mining Textual entailment Truecasing Word-sense disambiguation Word-sense induction Text segmentation Compound-term processing Lemmatisation Lexical analysis Text chunking Stemming Sentence segmentation Word segmentation\n",
      "\n",
      "LSA Summary:\n",
      "Discourse (semantics beyond individual sentences)[edit] Coreference resolution Given a sentence or larger chunk of text, determine which words (\"mentions\") refer to the same objects (\"entities\").\n",
      "Natural-language understanding (NLU) Convert chunks of text into more formal representations such as first-order logic structures that are easier for computer programs to manipulate.\n",
      "^ Yi, Chucai; Tian, Yingli (2012), \"Assistive Text Reading from Complex Background for Blind Persons\", Camera-Based Document Analysis and Recognition, Lecture Notes in Computer Science, vol.\n",
      "\n",
      "KL Summary:\n",
      "Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing.\n",
      "Common NLP tasks[edit] The following is a list of some of the most commonly researched tasks in natural language processing.\n",
      "The difficulty of this task depends greatly on the complexity of the morphology (i.e., the structure of words) of the language being considered.\n",
      "\n",
      "LexRank Summary:\n",
      "Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing.\n",
      "However, some written languages like Chinese, Japanese and Thai do not mark word boundaries in such a fashion, and in those languages text segmentation is a significant task requiring knowledge of the vocabulary and morphology of words in the language.\n",
      "Where RMM is the relative measure of meaning token is any block of text, sentence, phrase or word N is the number of tokens being analyzed PMM is the probable measure of meaning based on a corpora d is the non zero location of the token along the sequence of N tokens PF is the probability function specific to a language Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s.\n"
     ]
    }
   ],
   "source": [
    "def summarize_text(text, num_sentences=3):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "    \n",
    "    summarizers = {\n",
    "        \"Edmundson\": EdmundsonSummarizer(),\n",
    "        \"LSA\": LsaSummarizer(),\n",
    "        \"KL\": KLSummarizer(),\n",
    "        \"LexRank\": LexRankSummarizer()\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, summarizer in summarizers.items():\n",
    "        if name == \"Edmundson\":\n",
    "            summarizer.bonus_words = [\"NLP\", \"language\", \"algorithms\", \"AI\", \"natural\"]\n",
    "            summarizer.stigma_words = [\"is\", \"the\", \"a\", \"of\", \"and\", \"to\", \"in\"]\n",
    "            summarizer.null_words = [\"and\", \"or\", \"but\", \"if\", \"then\", \"with\", \"so\", \"on\"]\n",
    "        \n",
    "        summary = summarizer(parser.document, num_sentences)\n",
    "        results[name] = \"\\n\".join(str(sentence) for sentence in summary)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Function to summarize from a file\n",
    "def summarize_from_file(file_path, num_sentences=3):\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    return summarize_text(text, num_sentences)\n",
    "\n",
    "def summarize_from_url(url, num_sentences=3):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    return summarize_text(text, num_sentences)\n",
    "\n",
    "# Example usage\n",
    "document = \"\"\"\n",
    "Natural language processing is a subfield of artificial intelligence (AI) focused on the interaction between computers and humans through natural language. The ultimate objective of NLP is to enable computers to understand, interpret, and generate human languages in a way that is both valuable and meaningful. NLP is used to apply algorithms to identify and extract the natural language rules such that the unstructured language data is converted into a form that computers can understand.\n",
    "\"\"\"\n",
    "\n",
    "summaries = summarize_text(document)\n",
    "\n",
    "for name, summary in summaries.items():\n",
    "    print(f\"\\n{name} Summary:\")\n",
    "    print(summary)\n",
    "\n",
    "# Example usage for file and URL\n",
    "file_path = \"/home/t/Desktop/koulu/nltk/proj/test.in\"\n",
    "file_summaries = summarize_from_file(file_path)\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Natural_language_processing\"\n",
    "url_summaries = summarize_from_url(url)\n",
    "\n",
    "print(\"\\nFile Summaries:\")\n",
    "for name, summary in file_summaries.items():\n",
    "    print(f\"\\n{name} Summary:\")\n",
    "    print(summary)\n",
    "\n",
    "print(\"\\nURL Summaries:\")\n",
    "for name, summary in url_summaries.items():\n",
    "    print(f\"\\n{name} Summary:\")\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 samples from the dataset.\n",
      "Evaluating Edmundson summarizer...\n",
      "Evaluating LSA summarizer...\n",
      "Evaluating KL summarizer...\n",
      "Evaluating LexRank summarizer...\n",
      "Edmundson Summarizer:\n",
      "  ROUGE-1: 0.6785\n",
      "  ROUGE-2: 0.6186\n",
      "  ROUGE-L: 0.6780\n",
      "\n",
      "LSA Summarizer:\n",
      "  ROUGE-1: 0.4930\n",
      "  ROUGE-2: 0.3911\n",
      "  ROUGE-L: 0.4833\n",
      "\n",
      "KL Summarizer:\n",
      "  ROUGE-1: 0.4851\n",
      "  ROUGE-2: 0.3840\n",
      "  ROUGE-L: 0.4762\n",
      "\n",
      "LexRank Summarizer:\n",
      "  ROUGE-1: 0.5967\n",
      "  ROUGE-2: 0.5134\n",
      "  ROUGE-L: 0.5913\n",
      "\n",
      "Analysis:\n",
      "1. Performance Comparison:\n",
      "   - Best ROUGE-1 performance: Edmundson\n",
      "   - Best ROUGE-2 performance: Edmundson\n",
      "   - Best ROUGE-L performance: Edmundson\n"
     ]
    }
   ],
   "source": [
    "# Initialize ROUGE\n",
    "rouge = Rouge()\n",
    "\n",
    "# Initialize summarizers\n",
    "edmundson = EdmundsonSummarizer()\n",
    "edmundson.bonus_words = [\"important\", \"significant\", \"key\", \"central\", \"crucial\"]\n",
    "edmundson.stigma_words = [\"trivial\", \"minor\", \"unimportant\", \"insignificant\"]\n",
    "edmundson.null_words = [\"the\", \"a\", \"an\", \"in\", \"on\", \"at\", \"for\", \"of\", \"with\"]\n",
    "\n",
    "summarizers = {\n",
    "    \"Edmundson\": edmundson,\n",
    "    \"LSA\": LsaSummarizer(),\n",
    "    \"KL\": KLSummarizer(),\n",
    "    \"LexRank\": LexRankSummarizer()\n",
    "}\n",
    "\n",
    "def generate_summary(text, summarizer, num_sentences=3):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "    summary = summarizer(parser.document, num_sentences)\n",
    "    return \" \".join(str(sentence) for sentence in summary)\n",
    "\n",
    "def evaluate_summarizer(summarizer, data):\n",
    "    rouge_1_scores = []\n",
    "    rouge_2_scores = []\n",
    "    rouge_l_scores = []\n",
    "    \n",
    "    for _, row in data.iterrows():\n",
    "        full_text = row['summary']\n",
    "        reference_summary = row['title'] + \". \" + \" \".join(full_text.split()[:30])  # Use title and first 30 words as reference\n",
    "        generated_summary = generate_summary(full_text, summarizer)\n",
    "        \n",
    "        scores = rouge.get_scores(generated_summary, reference_summary)[0]\n",
    "        rouge_1_scores.append(scores['rouge-1']['f'])\n",
    "        rouge_2_scores.append(scores['rouge-2']['f'])\n",
    "        rouge_l_scores.append(scores['rouge-l']['f'])\n",
    "    \n",
    "    return np.mean(rouge_1_scores), np.mean(rouge_2_scores), np.mean(rouge_l_scores)\n",
    "\n",
    "# Load the Wikipedia Summary Dataset\n",
    "def load_wiki_summary_data(file_path, num_samples=10000):\n",
    "    with tarfile.open(file_path, \"r:gz\") as tar:\n",
    "        txt_file = [f for f in tar.getmembers() if f.name.endswith('.txt')][0]\n",
    "        with tar.extractfile(txt_file) as f:\n",
    "            content = io.TextIOWrapper(f, encoding='utf-8')\n",
    "            data = []\n",
    "            for i, line in enumerate(content):\n",
    "                if i >= num_samples:\n",
    "                    break\n",
    "                title, summary = line.strip().split('|||')\n",
    "                data.append({'title': title.strip(), 'summary': summary.strip()})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Load the dataset\n",
    "data = load_wiki_summary_data('raw.tar.gz', num_samples=10000)\n",
    "print(f\"Loaded {len(data)} samples from the dataset.\")\n",
    "\n",
    "# Evaluate each summarizer\n",
    "results = {}\n",
    "for name, summarizer in summarizers.items():\n",
    "    print(f\"Evaluating {name} summarizer...\")\n",
    "    rouge_1, rouge_2, rouge_l = evaluate_summarizer(summarizer, data)\n",
    "    results[name] = {'ROUGE-1': rouge_1, 'ROUGE-2': rouge_2, 'ROUGE-L': rouge_l}\n",
    "\n",
    "# Print results\n",
    "for name, scores in results.items():\n",
    "    print(f\"{name} Summarizer:\")\n",
    "    print(f\"  ROUGE-1: {scores['ROUGE-1']:.4f}\")\n",
    "    print(f\"  ROUGE-2: {scores['ROUGE-2']:.4f}\")\n",
    "    print(f\"  ROUGE-L: {scores['ROUGE-L']:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Analysis and comments\n",
    "print(\"Analysis:\")\n",
    "print(\"1. Performance Comparison:\")\n",
    "best_rouge1 = max(results, key=lambda x: results[x]['ROUGE-1'])\n",
    "best_rouge2 = max(results, key=lambda x: results[x]['ROUGE-2'])\n",
    "best_rougel = max(results, key=lambda x: results[x]['ROUGE-L'])\n",
    "print(f\"   - Best ROUGE-1 performance: {best_rouge1}\")\n",
    "print(f\"   - Best ROUGE-2 performance: {best_rouge2}\")\n",
    "print(f\"   - Best ROUGE-L performance: {best_rougel}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
